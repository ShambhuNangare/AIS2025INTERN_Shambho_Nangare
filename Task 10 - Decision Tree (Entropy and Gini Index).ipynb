{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a1882d",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edd18c",
   "metadata": {},
   "source": [
    "* A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It splits data into subsets based on feature values to make predictions.  \n",
    "* To decide where to split data, Decision Trees use impurity measures such as:      \n",
    "  * Entropy (Information Gain)\n",
    "  * Gini index\n",
    "* This measures help determine the best feature to split in by evaluating how \"pure\" and \"impure\" a node is.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7165ca3",
   "metadata": {},
   "source": [
    "# Entropy in Decisioin Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721bd584",
   "metadata": {},
   "source": [
    "* Entropy is measure of uncertainty or randomness in the dataset. It helps determine how mixed the classes are within a given node.\n",
    "  * If a dataset is pure (all samples belong to the same class), entropy is zero.\n",
    "  * if a dataset is completely impure (equal distribution of classed), entropy is maximum.\n",
    "  * The goal of a Decision Tree is to reduce entropy by splitting the dataset in a way that increases class purity.\n",
    "* To determine the best split, the tree calculates Information Gain, which measures how much entropy decreases after a split. A higher Information Gain means a better split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3dd4e",
   "metadata": {},
   "source": [
    "# Gini Index "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109eac9",
   "metadata": {},
   "source": [
    "* The Gini Index is another measure of used in Decision Trees. It represents the probability of randomly classifying a sample incorrectly if labels are randomly assigned based on the class distribution.\n",
    "  * A pure node has a Gini Index of Zero (all samples belong to one class).\n",
    "  * A node with equal distribution of classes has a higher Gini value, indicating more impuring.\n",
    "* Gini Index is computationally simpler than entropy, making it the the preferred criterion in many Decision Tree implementations.Unlike entropy, it does not involve logarithmic calculations, making it faster in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c481b0",
   "metadata": {},
   "source": [
    "# Why use Entropy and Gini Index?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95daa5e8",
   "metadata": {},
   "source": [
    "* Entropy and Gini Index help the Decision Tree identify the best feature for splitting data:\n",
    "  * Entropy (with Information Gain):  \n",
    "    ensures that the split maximizes information gained.\n",
    "  * Gini Index:  \n",
    "    Focuses on minimizing misclassification probability.\n",
    "* Both methods aim to create homogeneous subsets where most sapmples belong to the same class, improving the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfd75a",
   "metadata": {},
   "source": [
    "# What Do Entropy and Gini Index Do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa2ac4",
   "metadata": {},
   "source": [
    "* They measure impurity in the dataset.\n",
    "* They help the Decision Tree decide where to split the data.\n",
    "* The work by selecting the feature that leads to the most pure child nodes.\n",
    "* They improve classification accuracy by making the tree more efficient in seperating different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33295a",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6a35ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97b1db",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "147e1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87a1cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a Pandas DataFrame\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b2ea39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf996df",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d3e43e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6301e61",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e6e7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate the model\n",
    "def evaluate_decision_tree(criterion):\n",
    "    # Train the model with specified criterion (Gini or Entropy)\n",
    "    clf = DecisionTreeClassifier(random_state=42, criterion=criterion)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate precision, recall, and accuracy\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Create a dictionary to hold the results in a structured way\n",
    "    results = {\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall]\n",
    "    }\n",
    "\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Generate the classification report\n",
    "    classification_rep = classification_report(y_test, y_pred, target_names=wine.target_names, output_dict=True)\n",
    "    classification_df = pd.DataFrame(classification_rep).transpose()\n",
    "\n",
    "    return results_df, classification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a514d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using Gini\n",
    "gini_results, gini_classification_report = evaluate_decision_tree('gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2991bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using Entropy\n",
    "entropy_results, entropy_classification_report = evaluate_decision_tree('entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94b009f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Criteria - Model Performance Metrics:\n",
      "   Accuracy  Precision    Recall\n",
      "0  0.962963   0.963805  0.962963\n",
      "\n",
      "Gini Classification Report:\n",
      "              precision    recall  f1-score    support\n",
      "class_0        0.947368  0.947368  0.947368  19.000000\n",
      "class_1        0.954545  1.000000  0.976744  21.000000\n",
      "class_2        1.000000  0.928571  0.962963  14.000000\n",
      "accuracy       0.962963  0.962963  0.962963   0.962963\n",
      "macro avg      0.967305  0.958647  0.962359  54.000000\n",
      "weighted avg   0.963805  0.962963  0.962835  54.000000\n",
      "\n",
      "Entropy Criteria - Model Performance Metrics:\n",
      "   Accuracy  Precision    Recall\n",
      "0  0.851852   0.855205  0.851852\n",
      "\n",
      "Entropy Classification Report:\n",
      "              precision    recall  f1-score    support\n",
      "class_0        0.818182  0.947368  0.878049  19.000000\n",
      "class_1        0.894737  0.809524  0.850000  21.000000\n",
      "class_2        0.846154  0.785714  0.814815  14.000000\n",
      "accuracy       0.851852  0.851852  0.851852   0.851852\n",
      "macro avg      0.853024  0.847536  0.847621  54.000000\n",
      "weighted avg   0.855205  0.851852  0.850747  54.000000\n"
     ]
    }
   ],
   "source": [
    "# Display Results for Gini and Entropy\n",
    "print(\"Gini Criteria - Model Performance Metrics:\")\n",
    "print(gini_results)\n",
    "print(\"\\nGini Classification Report:\")\n",
    "print(gini_classification_report)\n",
    "\n",
    "print(\"\\nEntropy Criteria - Model Performance Metrics:\")\n",
    "print(entropy_results)\n",
    "print(\"\\nEntropy Classification Report:\")\n",
    "print(entropy_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997c499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
